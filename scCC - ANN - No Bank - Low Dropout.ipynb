{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import os\n",
    "import anndata\n",
    "import math\n",
    "import hnswlib\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from modules import network,mlp\n",
    "from utils import yaml_config_hook,save_model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "config = yaml_config_hook(\"config/config.yaml\")\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "args = parser.parse_args([])\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed_all(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "# np.random.seed(args.seed)\n",
    "class_num = args.classnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\anndata\\_core\\anndata.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if index_name in anno:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8569, 2000) (8569, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>41</th>\n",
       "      <th>45</th>\n",
       "      <th>62</th>\n",
       "      <th>68</th>\n",
       "      <th>106</th>\n",
       "      <th>133</th>\n",
       "      <th>147</th>\n",
       "      <th>...</th>\n",
       "      <th>19763</th>\n",
       "      <th>19786</th>\n",
       "      <th>19808</th>\n",
       "      <th>19854</th>\n",
       "      <th>19883</th>\n",
       "      <th>20021</th>\n",
       "      <th>20073</th>\n",
       "      <th>20109</th>\n",
       "      <th>20121</th>\n",
       "      <th>20124</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.302199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.637877</td>\n",
       "      <td>0.368960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.351171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.509045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     2        10   13   41   45        62       68  106  133  147  ...  19763  \\\n",
       "0  0.0  1.302199  0.0  0.0  0.0  0.000000  0.36896  0.0  0.0  0.0  ...    0.0   \n",
       "1  0.0  1.351171  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "2  0.0  0.000000  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "3  0.0  0.000000  0.0  0.0  0.0  0.711146  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "4  0.0  0.000000  0.0  0.0  0.0  0.509045  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "      19786  19808  19854    19883  20021  20073  20109     20121     20124  \n",
       "0  0.000000    0.0    0.0  0.00000    0.0    0.0    0.0  0.637877  0.368960  \n",
       "1  0.000000    0.0    0.0  0.00000    0.0    0.0    0.0  0.888292  0.000000  \n",
       "2  0.000000    0.0    0.0  0.00000    0.0    0.0    0.0  0.000000  0.000000  \n",
       "3  0.417500    0.0    0.0  0.93785    0.0    0.0    0.0  0.937850  0.000000  \n",
       "4  0.509045    0.0    0.0  0.00000    0.0    0.0    0.0  0.509045  0.509045  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "sparse_X = scipy.sparse.load_npz('data/filtered_Counts.npz')\n",
    "annoData = pd.read_table('data/annoData.txt')\n",
    "y = annoData[\"cellIden\"].to_numpy()\n",
    "high_var_gene = args.num_genes\n",
    "# normlization and feature selection\n",
    "adataSC = anndata.AnnData(X=sparse_X, obs=np.arange(sparse_X.shape[0]), var=np.arange(sparse_X.shape[1]))\n",
    "sc.pp.filter_genes(adataSC, min_cells=10)\n",
    "adataSC.raw = adataSC\n",
    "sc.pp.highly_variable_genes(adataSC, n_top_genes=high_var_gene, flavor='seurat_v3')\n",
    "sc.pp.normalize_total(adataSC, target_sum=1e4)\n",
    "sc.pp.log1p(adataSC)\n",
    "\n",
    "adataNorm = adataSC[:, adataSC.var.highly_variable]\n",
    "dataframe = adataNorm.to_df()\n",
    "x_ndarray = dataframe.values.squeeze()\n",
    "y_ndarray = np.expand_dims(y, axis=1)\n",
    "print(x_ndarray.shape,y_ndarray.shape)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1024\n",
      "1024\n",
      "2000\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,random_split,TensorDataset\n",
    "scDataset = TensorDataset(torch.tensor(x_ndarray, dtype=torch.float32),\n",
    "                              torch.tensor(y_ndarray, dtype=torch.float32))\n",
    "\n",
    "scDataLoader = DataLoader(scDataset, shuffle=True, batch_size=args.batch_size,drop_last=True)\n",
    "\n",
    "for features, labels in scDataLoader:\n",
    "    print(len(features[-1]))\n",
    "    print(len(features))\n",
    "    print(len(labels))\n",
    "    break\n",
    "\n",
    "scGenDataLoader = DataLoader(scDataset, shuffle=False, batch_size=args.batch_size,drop_last=False)\n",
    "\n",
    "for features, labels in scDataLoader:\n",
    "    print(len(features[-1]))\n",
    "    print(len(features))\n",
    "    print(len(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8569 2000\n"
     ]
    }
   ],
   "source": [
    "num_elements=len(scDataset)\n",
    "dim=high_var_gene\n",
    "print(num_elements,dim)\n",
    "p=hnswlib.Index(space='cosine',dim=dim)\n",
    "p.init_index(max_elements=num_elements, ef_construction=100, M=16)\n",
    "p.set_ef(100)\n",
    "p.set_num_threads(4)\n",
    "p.add_items(x_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "mlp = mlp.MLP(num_genes=args.num_genes)\n",
    "model = network.Network(mlp, args.feature_dim, args.classnum)\n",
    "model = model.to('cuda')\n",
    "# optimizer / loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class InstanceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,batch_size,temperature,device):\n",
    "        super(InstanceLoss,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.temperature=temperature\n",
    "        self.device=device\n",
    "\n",
    "        self.mask=self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "\n",
    "    def mask_correlated_samples(self,batch_size):\n",
    "        N=2*batch_size\n",
    "        mask=torch.ones(N,N)\n",
    "        mask=mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i,batch_size+1]=0\n",
    "            mask[batch_size+i,i]=0\n",
    "        mask=mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self,z_i,z_j):\n",
    "        N=2*self.batch_size\n",
    "        z=torch.cat((z_i,z_j),dim=0)\n",
    "\n",
    "        sim=torch.matmul(z,z.T)/self.temperature\n",
    "        sim_i_j=torch.diag(sim,self.batch_size)\n",
    "        sim_j_i=torch.diag(sim,-self.batch_size)\n",
    "\n",
    "        positive_samples= torch.cat((sim_i_j,sim_j_i),dim=0).reshape(N,1)\n",
    "        negative_samples=sim[self.mask].reshape(N,-1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ClusterLoss(nn.Module):\n",
    "    def __init__(self, class_num, temperature, device):\n",
    "        super(ClusterLoss, self).__init__()\n",
    "        self.class_num = class_num\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "\n",
    "        self.mask = self.mask_correlated_clusters(class_num)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_clusters(self, class_num):\n",
    "        N = 2 * class_num\n",
    "        mask = torch.ones((N, N))\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(class_num):\n",
    "            mask[i, class_num + i] = 0\n",
    "            mask[class_num + i, i] = 0\n",
    "        mask = mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, c_i, c_j):\n",
    "        p_i = c_i.sum(0).view(-1)\n",
    "        p_i /= p_i.sum()\n",
    "        ne_i = math.log(p_i.size(0)) + (p_i * torch.log(p_i)).sum()\n",
    "        p_j = c_j.sum(0).view(-1)\n",
    "        p_j /= p_j.sum()\n",
    "        ne_j = math.log(p_j.size(0)) + (p_j * torch.log(p_j)).sum()\n",
    "        ne_loss = ne_i + ne_j\n",
    "\n",
    "        c_i = c_i.t()\n",
    "        c_j = c_j.t()\n",
    "        N = 2 * self.class_num\n",
    "        c = torch.cat((c_i, c_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(c.unsqueeze(1), c.unsqueeze(0)) / self.temperature\n",
    "        sim_i_j = torch.diag(sim, self.class_num)\n",
    "        sim_j_i = torch.diag(sim, -self.class_num)\n",
    "\n",
    "        positive_clusters = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_clusters = sim[self.mask].reshape(N, -1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_clusters.device).long()\n",
    "        logits = torch.cat((positive_clusters, negative_clusters), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss + ne_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank():\n",
    "\n",
    "    # 初始化，传入参数\n",
    "    def __init__(self,batch_size,full_data,topK=10):\n",
    "        self.topK=topK\n",
    "        self.batch_size=batch_size\n",
    "        self.bank=None\n",
    "        self.full_data=full_data\n",
    "\n",
    "    # 根据在updateBank中更新的hnsw对象以及输入的数据data（这里可以是embedding）提取TopK个近邻的数据\n",
    "    # 返回的结果是一个形状为[TopK,batch_size,num_genes]的数组，从第一个维度来看，\n",
    "    # 每个[batch_size,num_genes]的子数组都是根据输入的数据data寻找的一个近邻，一共TopK个\n",
    "    def generateContrast(self,data):\n",
    "        if self.bank is not None:\n",
    "            contrasts=np.empty((self.topK,args.batch_size,args.num_genes))\n",
    "            labels,distances=self.bank.knn_query(data,k=self.topK)\n",
    "            \n",
    "            # print(labels)\n",
    "\n",
    "            for step,label in enumerate(labels):\n",
    "                contrasts[:,step]=self.full_data[label.tolist()]\n",
    "            return contrasts\n",
    "        else:\n",
    "            print('Memory Bank has not been initialized......')\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    # 根据输入的embedding更新hnsw对象\n",
    "    def updateBank(self,embedding):\n",
    "        num_elements=len(embedding)\n",
    "        dim=embedding.shape[1]\n",
    "        self.bank=hnswlib.Index(space='cosine',dim=dim)\n",
    "        self.bank.init_index(max_elements=num_elements, ef_construction=100, M=16)\n",
    "        self.bank.set_ef(100)\n",
    "        self.bank.set_num_threads(4)\n",
    "        self.bank.add_items(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def train(instance_loss,cluster_loss):\n",
    "#     loss_epoch = 0\n",
    "#     for step, (data,label) in enumerate(scDataset):\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         x_i=data.clone().numpy()\n",
    "        \n",
    "#         labels, distances = p.knn_query(x_i, k=31)\n",
    "#         labels=labels.squeeze()\n",
    "#         contrastive=x_ndarray[labels.tolist()]\n",
    "#         input=np.row_stack((contrastive,x_i))\n",
    "#         input_1=input.copy()\n",
    "#         np.random.shuffle(input_1)\n",
    "#         x_a=torch.from_numpy(input).to('cuda')\n",
    "#         x_b=torch.from_numpy(input_1).to('cuda')\n",
    "        \n",
    "#         z_i,z_j,c_i,c_j=model(x_a,x_b)\n",
    "#         loss_instance=instance_loss(z_i,z_j)\n",
    "#         loss_cluster=cluster_loss(c_i,c_j)\n",
    "#         loss = loss_instance + loss_cluster\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if step % 1000 == 0:\n",
    "#             print(f\"Step [{step}/{len(scDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n",
    "#         loss_epoch += loss.item()\n",
    "#     return loss_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(instance_loss,cluster_loss,memory_bank,model,epoch):\n",
    "    loss_epoch=0\n",
    "    instance_losses=[]\n",
    "    cluster_losses=[]\n",
    "    for step,(data,label) in enumerate(scDataLoader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # if epoch<20:\n",
    "        embedding_data=data.numpy().copy()\n",
    "        data=data.to('cuda')\n",
    "        # else:\n",
    "        #     data=data.to('cuda')\n",
    "        #     embedding_data=model.forward_embedding(data).cpu().detach().numpy()    \n",
    "        contrast_samples=memory_bank.generateContrast(embedding_data)\n",
    "        iter_times=contrast_samples.shape[0]\n",
    "        for sample in contrast_samples:\n",
    "            # print(f'sample shape:\\n{sample.shape}')\n",
    "            # print(f'data shape:\\n{data.shape}')\n",
    "            x_i=data.clone()\n",
    "            x_j=torch.tensor(sample,dtype=torch.float32).to('cuda')\n",
    "\n",
    "            z_i,z_j,c_i,c_j=model(x_i,x_j)\n",
    "            loss_instance=instance_loss(z_i,z_j)\n",
    "            loss_cluster=cluster_loss(c_i,c_j)\n",
    "            loss = loss_instance + loss_cluster\n",
    "            # print(f'------ loss:\\n{loss}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch+=loss.item()\n",
    "            instance_losses.append(loss_instance.item())\n",
    "            cluster_losses.append(loss_cluster.item())\n",
    "        if step % 2 == 0:\n",
    "                print(f\"Step [{step}/{len(scDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n",
    "            \n",
    "    return loss_epoch/iter_times,instance_losses,cluster_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def inference(loader, model, device):\n",
    "    model.eval()\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            c = model.forward_cluster(x)\n",
    "        c = c.detach()\n",
    "        feature_vector.extend(c.cpu().detach().numpy())\n",
    "        labels_vector.extend(y.numpy())\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    # print(feature_vector.shape, labels_vector.shape)\n",
    "    return feature_vector, labels_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evaluation import evaluation\n",
    "def test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X, Y = inference(scDataLoader, model, device)\n",
    "    # print(X.shape,Y.shape)\n",
    "    nmi, ari, f, acc = evaluation.evaluate(Y.reshape(-1), X)\n",
    "    # print('NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    return nmi, ari, f, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/8]\t loss_instance: 7.607870101928711\t loss_cluster: 3.2965989112854004\n",
      "Step [2/8]\t loss_instance: 7.3649067878723145\t loss_cluster: 3.295680284500122\n",
      "Step [4/8]\t loss_instance: 7.092578411102295\t loss_cluster: 3.288856029510498\n",
      "Step [6/8]\t loss_instance: 7.014703750610352\t loss_cluster: 3.263911485671997\n",
      "\n",
      "Epoch [1/500]\t Loss: 10.548831224441528 \n",
      "\n",
      "Step [0/8]\t Computing features...\n",
      "Features shape (8192,)\n",
      "14\n",
      "Test NMI = 0.4505 ARI = 0.3684 F = 0.5030 ACC = 0.0090\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/8]\t loss_instance: 6.863243579864502\t loss_cluster: 3.1942138671875\n",
      "Step [2/8]\t loss_instance: 6.69709587097168\t loss_cluster: 3.092392921447754\n",
      "Step [4/8]\t loss_instance: 6.629603385925293\t loss_cluster: 3.0146377086639404\n",
      "Step [6/8]\t loss_instance: 6.571809768676758\t loss_cluster: 2.897670030593872\n",
      "\n",
      "Epoch [2/500]\t Loss: 9.719462752342224 \n",
      "\n",
      "Step [0/8]\t Computing features...\n",
      "Features shape (8192,)\n",
      "14\n",
      "Test NMI = 0.5377 ARI = 0.3283 F = 0.5065 ACC = 0.4807\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/8]\t loss_instance: 6.494650363922119\t loss_cluster: 2.730250835418701\n",
      "Step [2/8]\t loss_instance: 6.456292152404785\t loss_cluster: 2.6106977462768555\n",
      "Step [4/8]\t loss_instance: 6.42783260345459\t loss_cluster: 2.550056219100952\n",
      "Step [6/8]\t loss_instance: 6.438074588775635\t loss_cluster: 2.4967987537384033\n",
      "\n",
      "Epoch [3/500]\t Loss: 9.04032715884122 \n",
      "\n",
      "Step [0/8]\t Computing features...\n",
      "Features shape (8192,)\n",
      "14\n",
      "Test NMI = 0.6958 ARI = 0.6489 F = 0.7161 ACC = 0.7073\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/8]\t loss_instance: 6.401851177215576\t loss_cluster: 2.4253392219543457\n",
      "Step [2/8]\t loss_instance: 6.363900661468506\t loss_cluster: 2.391407012939453\n",
      "Step [4/8]\t loss_instance: 6.338507175445557\t loss_cluster: 2.328218698501587\n",
      "Step [6/8]\t loss_instance: 6.345836639404297\t loss_cluster: 2.265901565551758\n",
      "\n",
      "Epoch [4/500]\t Loss: 8.695131637833335 \n",
      "\n",
      "Step [0/8]\t Computing features...\n",
      "Features shape (8192,)\n",
      "14\n",
      "Test NMI = 0.6538 ARI = 0.4605 F = 0.5583 ACC = 0.5328\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/8]\t loss_instance: 6.30307674407959\t loss_cluster: 2.211395263671875\n",
      "Step [2/8]\t loss_instance: 6.3028435707092285\t loss_cluster: 2.186570405960083\n",
      "Step [4/8]\t loss_instance: 6.297037124633789\t loss_cluster: 2.1614034175872803\n",
      "Step [6/8]\t loss_instance: 6.282373428344727\t loss_cluster: 2.1257500648498535\n",
      "\n",
      "Epoch [5/500]\t Loss: 8.45867617563768 \n",
      "\n",
      "Step [0/8]\t Computing features...\n",
      "Features shape (8192,)\n",
      "14\n",
      "Test NMI = 0.6674 ARI = 0.5093 F = 0.6046 ACC = 0.5780\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/8]\t loss_instance: 6.263211250305176\t loss_cluster: 2.1103219985961914\n",
      "Step [2/8]\t loss_instance: 6.248765468597412\t loss_cluster: 2.085684299468994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\GitHub\\scCC\\scCC - ANN - No Bank - Low Dropout.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=15'>16</a>\u001b[0m lr \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=16'>17</a>\u001b[0m \u001b[39m# if epoch %20 ==0:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=17'>18</a>\u001b[0m \u001b[39m#     embeddings=np.empty((0,mlp.rep_dim))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=18'>19</a>\u001b[0m \u001b[39m#     model.eval()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=23'>24</a>\u001b[0m \u001b[39m#             embeddings=np.row_stack((embeddings,embedding))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=24'>25</a>\u001b[0m \u001b[39m#             memory_bank.updateBank(embeddings)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=25'>26</a>\u001b[0m loss_epoch,instance_loss_item,cluster_loss_item \u001b[39m=\u001b[39m train(instance_loss,cluster_loss,memory_bank,epoch\u001b[39m=\u001b[39;49mepoch,model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=26'>27</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss_epoch\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(scDataLoader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000013?line=27'>28</a>\u001b[0m instance_losses\u001b[39m.\u001b[39mappend(instance_loss_item)\n",
      "\u001b[1;32md:\\Documents\\GitHub\\scCC\\scCC - ANN - No Bank - Low Dropout.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(instance_loss, cluster_loss, memory_bank, model, epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=9'>10</a>\u001b[0m data\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=10'>11</a>\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=11'>12</a>\u001b[0m \u001b[39m#     data=data.to('cuda')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=12'>13</a>\u001b[0m \u001b[39m#     embedding_data=model.forward_embedding(data).cpu().detach().numpy()    \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=13'>14</a>\u001b[0m contrast_samples\u001b[39m=\u001b[39mmemory_bank\u001b[39m.\u001b[39;49mgenerateContrast(embedding_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=14'>15</a>\u001b[0m iter_times\u001b[39m=\u001b[39mcontrast_samples\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m contrast_samples:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=16'>17</a>\u001b[0m     \u001b[39m# print(f'sample shape:\\n{sample.shape}')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000010?line=17'>18</a>\u001b[0m     \u001b[39m# print(f'data shape:\\n{data.shape}')\u001b[39;00m\n",
      "\u001b[1;32md:\\Documents\\GitHub\\scCC\\scCC - ANN - No Bank - Low Dropout.ipynb Cell 9'\u001b[0m in \u001b[0;36mMemoryBank.generateContrast\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000008?line=17'>18</a>\u001b[0m     \u001b[39m# print(labels)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000008?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m step,label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(labels):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000008?line=20'>21</a>\u001b[0m         contrasts[:,step]\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfull_data[label\u001b[39m.\u001b[39;49mtolist()]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000008?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m contrasts\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN%20-%20No%20Bank%20-%20Low%20Dropout.ipynb#ch0000008?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_device=torch.device('cuda')\n",
    "instance_loss=InstanceLoss(batch_size=args.batch_size,temperature=0.5,device=loss_device)\n",
    "cluster_loss=ClusterLoss(class_num=class_num,temperature=0.5,device=loss_device)\n",
    "accs=[]\n",
    "losses=[]\n",
    "aris=[]\n",
    "nmis=[]\n",
    "fs=[]\n",
    "instance_losses=[]\n",
    "cluster_losses=[]\n",
    "\n",
    "memory_bank=MemoryBank(batch_size=args.batch_size,full_data=x_ndarray,topK=11)\n",
    "memory_bank.updateBank(x_ndarray)\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    # if epoch %20 ==0:\n",
    "    #     embeddings=np.empty((0,mlp.rep_dim))\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         for step, (x, y) in enumerate(scGenDataLoader):\n",
    "    #             x=x.to('cuda')\n",
    "    #             embedding=model.forward_embedding(x).cpu().detach().numpy()                 \n",
    "    #             embeddings=np.row_stack((embeddings,embedding))\n",
    "    #             memory_bank.updateBank(embeddings)\n",
    "    loss_epoch,instance_loss_item,cluster_loss_item = train(instance_loss,cluster_loss,memory_bank,epoch=epoch,model=model)\n",
    "    losses.append(loss_epoch/len(scDataLoader))\n",
    "    instance_losses.append(instance_loss_item)\n",
    "    cluster_losses.append(cluster_loss_item)\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        save_model(args, model, optimizer, epoch)\n",
    "    print(f\"\\nEpoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(scDataLoader)} \\n\")\n",
    "    nmi, ari, f, acc = test()\n",
    "    accs.append(acc)\n",
    "    aris.append(ari)\n",
    "    nmis.append(nmi)\n",
    "    fs.append(f)\n",
    "    print('Test NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    print('========'*8+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "ax[0].plot(accs)\n",
    "ax[0].set_title('Accuracy during Validation')\n",
    "ax[1].plot(losses)\n",
    "ax[1].set_title('Loss during Training')\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.suptitle('Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "ax[0].plot(nmis)\n",
    "ax[0].set_title('Test NMI')\n",
    "ax[1].plot(aris)\n",
    "ax[1].set_title('Test ARI')\n",
    "ax[2].plot(fs)\n",
    "ax[2].set_title('Test F')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.suptitle('Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(instance_losses)\n",
    "ax[0].set_title('Instance Loss')\n",
    "ax[1].plot(cluster_losses)\n",
    "ax[1].set_title('Cluster Loss')\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.suptitle('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_loss_cat=[]\n",
    "for i in instance_losses:\n",
    "    instance_loss_cat.extend(i)\n",
    "\n",
    "cluster_loss_cat=[]\n",
    "for i in cluster_losses:\n",
    "    cluster_loss_cat.extend(i)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(120, 60))\n",
    "ax.plot(instance_loss_cat)\n",
    "ax.set_title('Instance Loss')\n",
    "ax.plot(cluster_loss_cat)\n",
    "ax.set_title('Cluster Loss')\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.suptitle('Training Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
