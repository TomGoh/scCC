{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import os\n",
    "import anndata\n",
    "import math\n",
    "import hnswlib\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from modules import network,mlp\n",
    "from utils import yaml_config_hook,save_model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "config = yaml_config_hook(\"config/config.yaml\")\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "args = parser.parse_args([])\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed_all(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "# np.random.seed(args.seed)\n",
    "class_num = args.classnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\anndata\\_core\\anndata.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if index_name in anno:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8569, 3000) (8569, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>33</th>\n",
       "      <th>41</th>\n",
       "      <th>45</th>\n",
       "      <th>47</th>\n",
       "      <th>62</th>\n",
       "      <th>68</th>\n",
       "      <th>...</th>\n",
       "      <th>19854</th>\n",
       "      <th>19871</th>\n",
       "      <th>19883</th>\n",
       "      <th>20021</th>\n",
       "      <th>20040</th>\n",
       "      <th>20073</th>\n",
       "      <th>20108</th>\n",
       "      <th>20109</th>\n",
       "      <th>20121</th>\n",
       "      <th>20124</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.302199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.637877</td>\n",
       "      <td>0.368960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.351171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.93785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.509045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     2    4        10   13   33   41   45   47        62       68  ...  19854  \\\n",
       "0  0.0  0.0  1.302199  0.0  0.0  0.0  0.0  0.0  0.000000  0.36896  ...    0.0   \n",
       "1  0.0  0.0  1.351171  0.0  0.0  0.0  0.0  0.0  0.000000  0.00000  ...    0.0   \n",
       "2  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.00000  ...    0.0   \n",
       "3  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.711146  0.00000  ...    0.0   \n",
       "4  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.509045  0.00000  ...    0.0   \n",
       "\n",
       "      19871    19883  20021  20040  20073    20108  20109     20121     20124  \n",
       "0  0.000000  0.00000    0.0    0.0    0.0  0.36896    0.0  0.637877  0.368960  \n",
       "1  0.000000  0.00000    0.0    0.0    0.0  0.00000    0.0  0.888292  0.000000  \n",
       "2  0.000000  0.00000    0.0    0.0    0.0  0.00000    0.0  0.000000  0.000000  \n",
       "3  0.000000  0.93785    0.0    0.0    0.0  0.00000    0.0  0.937850  0.000000  \n",
       "4  0.509045  0.00000    0.0    0.0    0.0  0.00000    0.0  0.509045  0.509045  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "sparse_X = scipy.sparse.load_npz('data/filtered_Counts.npz')\n",
    "annoData = pd.read_table('data/annoData.txt')\n",
    "y = annoData[\"cellIden\"].to_numpy()\n",
    "high_var_gene = args.num_genes\n",
    "# normlization and feature selection\n",
    "adataSC = anndata.AnnData(X=sparse_X, obs=np.arange(sparse_X.shape[0]), var=np.arange(sparse_X.shape[1]))\n",
    "sc.pp.filter_genes(adataSC, min_cells=10)\n",
    "adataSC.raw = adataSC\n",
    "sc.pp.highly_variable_genes(adataSC, n_top_genes=high_var_gene, flavor='seurat_v3')\n",
    "sc.pp.normalize_total(adataSC, target_sum=1e4)\n",
    "sc.pp.log1p(adataSC)\n",
    "\n",
    "adataNorm = adataSC[:, adataSC.var.highly_variable]\n",
    "dataframe = adataNorm.to_df()\n",
    "x_ndarray = dataframe.values.squeeze()\n",
    "y_ndarray = np.expand_dims(y, axis=1)\n",
    "print(x_ndarray.shape,y_ndarray.shape)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,random_split,TensorDataset\n",
    "scDataset = TensorDataset(torch.tensor(x_ndarray, dtype=torch.float32),\n",
    "                              torch.tensor(y_ndarray, dtype=torch.float32))\n",
    "\n",
    "scDataLoader = DataLoader(scDataset, shuffle=True, batch_size=1,drop_last=True)\n",
    "\n",
    "for features, labels in scDataLoader:\n",
    "    print(len(features[-1]))\n",
    "    print(len(features))\n",
    "    print(len(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8569 3000\n"
     ]
    }
   ],
   "source": [
    "num_elements=len(scDataset)\n",
    "dim=high_var_gene\n",
    "print(num_elements,dim)\n",
    "p=hnswlib.Index(space='cosine',dim=dim)\n",
    "p.init_index(max_elements=num_elements, ef_construction=100, M=16)\n",
    "p.set_ef(100)\n",
    "p.set_num_threads(4)\n",
    "p.add_items(x_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "mlp = mlp.MLP(num_genes=args.num_genes)\n",
    "model = network.Network(mlp, args.feature_dim, args.classnum)\n",
    "model = model.to('cuda')\n",
    "# optimizer / loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class InstanceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,batch_size,temperature,device):\n",
    "        super(InstanceLoss,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.temperature=temperature\n",
    "        self.device=device\n",
    "\n",
    "        self.mask=self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "\n",
    "    def mask_correlated_samples(self,batch_size):\n",
    "        N=2*batch_size\n",
    "        mask=torch.ones(N,N)\n",
    "        mask=mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i,batch_size+1]=0\n",
    "            mask[batch_size+i,i]=0\n",
    "        mask=mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self,z_i,z_j):\n",
    "        N=2*self.batch_size\n",
    "        z=torch.cat((z_i,z_j),dim=0)\n",
    "\n",
    "        sim=torch.matmul(z,z.T)/self.temperature\n",
    "        sim_i_j=torch.diag(sim,self.batch_size)\n",
    "        sim_j_i=torch.diag(sim,-self.batch_size)\n",
    "\n",
    "        positive_samples= torch.cat((sim_i_j,sim_j_i),dim=0).reshape(N,1)\n",
    "        negative_samples=sim[self.mask].reshape(N,-1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ClusterLoss(nn.Module):\n",
    "    def __init__(self, class_num, temperature, device):\n",
    "        super(ClusterLoss, self).__init__()\n",
    "        self.class_num = class_num\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "\n",
    "        self.mask = self.mask_correlated_clusters(class_num)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_clusters(self, class_num):\n",
    "        N = 2 * class_num\n",
    "        mask = torch.ones((N, N))\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(class_num):\n",
    "            mask[i, class_num + i] = 0\n",
    "            mask[class_num + i, i] = 0\n",
    "        mask = mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, c_i, c_j):\n",
    "        p_i = c_i.sum(0).view(-1)\n",
    "        p_i /= p_i.sum()\n",
    "        ne_i = math.log(p_i.size(0)) + (p_i * torch.log(p_i)).sum()\n",
    "        p_j = c_j.sum(0).view(-1)\n",
    "        p_j /= p_j.sum()\n",
    "        ne_j = math.log(p_j.size(0)) + (p_j * torch.log(p_j)).sum()\n",
    "        ne_loss = ne_i + ne_j\n",
    "\n",
    "        c_i = c_i.t()\n",
    "        c_j = c_j.t()\n",
    "        N = 2 * self.class_num\n",
    "        c = torch.cat((c_i, c_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(c.unsqueeze(1), c.unsqueeze(0)) / self.temperature\n",
    "        sim_i_j = torch.diag(sim, self.class_num)\n",
    "        sim_j_i = torch.diag(sim, -self.class_num)\n",
    "\n",
    "        positive_clusters = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_clusters = sim[self.mask].reshape(N, -1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_clusters.device).long()\n",
    "        logits = torch.cat((positive_clusters, negative_clusters), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss + ne_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from preprocessing import transformation\n",
    "def train(instance_loss,cluster_loss):\n",
    "    loss_epoch = 0\n",
    "    for step, (data,label) in enumerate(scDataset):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_i=data.clone().numpy()\n",
    "        \n",
    "        labels, distances = p.knn_query(x_i, k=63)\n",
    "        labels=labels.squeeze()\n",
    "        contrastive=x_ndarray[labels.tolist()]\n",
    "        input=np.row_stack((contrastive,x_ndarray[0]))\n",
    "        input_1=input.copy()\n",
    "        np.random.shuffle(input_1)\n",
    "        x_i=torch.from_numpy(input).to('cuda')\n",
    "        x_j=torch.from_numpy(input_1).to('cuda')\n",
    "        \n",
    "        z_i,z_j,c_i,c_j=model(x_i,x_j)\n",
    "        loss_instance=instance_loss(z_i,z_j)\n",
    "        loss_cluster=cluster_loss(c_i,c_j)\n",
    "        loss = loss_instance + loss_cluster\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                f\"Step [{step}/{len(scDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def inference(loader, model, device):\n",
    "    model.eval()\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            c = model.forward_cluster(x)\n",
    "        c = c.detach()\n",
    "        feature_vector.extend(c.cpu().detach().numpy())\n",
    "        labels_vector.extend(y.numpy())\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    # print(feature_vector.shape, labels_vector.shape)\n",
    "    return feature_vector, labels_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evaluation import evaluation\n",
    "def test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X, Y = inference(scDataLoader, model, device)\n",
    "    # print(X.shape,Y.shape)\n",
    "    nmi, ari, f, acc = evaluation.evaluate(Y.reshape(-1), X)\n",
    "    # print('NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    return nmi, ari, f, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/8569]\t loss_instance: 4.844147682189941\t loss_cluster: 3.2991716861724854\n",
      "Step [10/8569]\t loss_instance: 4.844319820404053\t loss_cluster: 3.297469139099121\n",
      "Step [20/8569]\t loss_instance: 4.844188213348389\t loss_cluster: 3.296834707260132\n",
      "Step [30/8569]\t loss_instance: 4.844030857086182\t loss_cluster: 3.2961859703063965\n",
      "Step [40/8569]\t loss_instance: 4.84431266784668\t loss_cluster: 3.2959725856781006\n",
      "Step [50/8569]\t loss_instance: 4.844130039215088\t loss_cluster: 3.2958784103393555\n",
      "Step [60/8569]\t loss_instance: 4.844621658325195\t loss_cluster: 3.295893907546997\n",
      "Step [70/8569]\t loss_instance: 4.844230651855469\t loss_cluster: 3.2958688735961914\n",
      "Step [80/8569]\t loss_instance: 4.844424724578857\t loss_cluster: 3.295849323272705\n",
      "Step [90/8569]\t loss_instance: 4.84429931640625\t loss_cluster: 3.2958457469940186\n",
      "Step [100/8569]\t loss_instance: 4.845033645629883\t loss_cluster: 3.2958714962005615\n",
      "Step [110/8569]\t loss_instance: 4.844419956207275\t loss_cluster: 3.2958433628082275\n",
      "Step [120/8569]\t loss_instance: 4.843947410583496\t loss_cluster: 3.2958428859710693\n",
      "Step [130/8569]\t loss_instance: 4.843008041381836\t loss_cluster: 3.295835256576538\n",
      "Step [140/8569]\t loss_instance: 4.846180438995361\t loss_cluster: 3.2958459854125977\n",
      "Step [150/8569]\t loss_instance: 4.845977783203125\t loss_cluster: 3.2958452701568604\n",
      "Step [160/8569]\t loss_instance: 4.844701290130615\t loss_cluster: 3.295853853225708\n",
      "Step [170/8569]\t loss_instance: 4.8444294929504395\t loss_cluster: 3.295875310897827\n",
      "Step [180/8569]\t loss_instance: 4.844377517700195\t loss_cluster: 3.2958388328552246\n",
      "Step [190/8569]\t loss_instance: 4.84419059753418\t loss_cluster: 3.2958438396453857\n",
      "Step [200/8569]\t loss_instance: 4.844269275665283\t loss_cluster: 3.2958457469940186\n",
      "Step [210/8569]\t loss_instance: 4.844398498535156\t loss_cluster: 3.295839548110962\n",
      "Step [220/8569]\t loss_instance: 4.844298362731934\t loss_cluster: 3.29585337638855\n",
      "Step [230/8569]\t loss_instance: 4.844730854034424\t loss_cluster: 3.2958385944366455\n",
      "Step [240/8569]\t loss_instance: 4.844091892242432\t loss_cluster: 3.29583740234375\n",
      "Step [250/8569]\t loss_instance: 4.844521522521973\t loss_cluster: 3.2958450317382812\n",
      "Step [260/8569]\t loss_instance: 4.8437299728393555\t loss_cluster: 3.2958405017852783\n",
      "Step [270/8569]\t loss_instance: 4.843715190887451\t loss_cluster: 3.29583740234375\n",
      "Step [280/8569]\t loss_instance: 4.844428539276123\t loss_cluster: 3.29584002494812\n",
      "Step [290/8569]\t loss_instance: 4.843980312347412\t loss_cluster: 3.295842409133911\n",
      "Step [300/8569]\t loss_instance: 4.8443522453308105\t loss_cluster: 3.295841693878174\n",
      "Step [310/8569]\t loss_instance: 4.843852996826172\t loss_cluster: 3.295840263366699\n",
      "Step [320/8569]\t loss_instance: 4.844170570373535\t loss_cluster: 3.295848846435547\n",
      "Step [330/8569]\t loss_instance: 4.8443603515625\t loss_cluster: 3.295868158340454\n",
      "Step [340/8569]\t loss_instance: 4.843924045562744\t loss_cluster: 3.2958428859710693\n",
      "Step [350/8569]\t loss_instance: 4.844522476196289\t loss_cluster: 3.2958359718322754\n",
      "Step [360/8569]\t loss_instance: 4.843574047088623\t loss_cluster: 3.2958366870880127\n",
      "Step [370/8569]\t loss_instance: 4.844185829162598\t loss_cluster: 3.29585337638855\n",
      "Step [380/8569]\t loss_instance: 4.84395170211792\t loss_cluster: 3.295841693878174\n",
      "Step [390/8569]\t loss_instance: 4.844361305236816\t loss_cluster: 3.295844316482544\n",
      "Step [400/8569]\t loss_instance: 4.844705104827881\t loss_cluster: 3.295839786529541\n",
      "Step [410/8569]\t loss_instance: 4.843172550201416\t loss_cluster: 3.2958433628082275\n",
      "Step [420/8569]\t loss_instance: 4.844717025756836\t loss_cluster: 3.2958436012268066\n",
      "Step [430/8569]\t loss_instance: 4.8443732261657715\t loss_cluster: 3.2958357334136963\n",
      "Step [440/8569]\t loss_instance: 4.8440141677856445\t loss_cluster: 3.295842170715332\n",
      "Step [450/8569]\t loss_instance: 4.842186450958252\t loss_cluster: 3.295851230621338\n",
      "Step [460/8569]\t loss_instance: 4.8445048332214355\t loss_cluster: 3.2958574295043945\n",
      "Step [470/8569]\t loss_instance: 4.844478607177734\t loss_cluster: 3.2958574295043945\n",
      "Step [480/8569]\t loss_instance: 4.844160079956055\t loss_cluster: 3.2958412170410156\n",
      "Step [490/8569]\t loss_instance: 4.843104362487793\t loss_cluster: 3.295834541320801\n",
      "Step [500/8569]\t loss_instance: 4.844539165496826\t loss_cluster: 3.295849561691284\n",
      "Step [510/8569]\t loss_instance: 4.844072341918945\t loss_cluster: 3.2958569526672363\n",
      "Step [520/8569]\t loss_instance: 4.845580577850342\t loss_cluster: 3.2958383560180664\n",
      "Step [530/8569]\t loss_instance: 4.844367027282715\t loss_cluster: 3.2958405017852783\n",
      "Step [540/8569]\t loss_instance: 4.84326696395874\t loss_cluster: 3.29583740234375\n",
      "Step [550/8569]\t loss_instance: 4.843205451965332\t loss_cluster: 3.2958385944366455\n",
      "Step [560/8569]\t loss_instance: 4.842313289642334\t loss_cluster: 3.2958333492279053\n",
      "Step [570/8569]\t loss_instance: 4.843590259552002\t loss_cluster: 3.2958478927612305\n",
      "Step [580/8569]\t loss_instance: 4.842766761779785\t loss_cluster: 3.295841932296753\n",
      "Step [590/8569]\t loss_instance: 4.843098163604736\t loss_cluster: 3.295835494995117\n",
      "Step [600/8569]\t loss_instance: 4.846020698547363\t loss_cluster: 3.2958450317382812\n",
      "Step [610/8569]\t loss_instance: 4.844265460968018\t loss_cluster: 3.29584002494812\n",
      "Step [620/8569]\t loss_instance: 4.843575477600098\t loss_cluster: 3.295835018157959\n",
      "Step [630/8569]\t loss_instance: 4.843760013580322\t loss_cluster: 3.2958390712738037\n",
      "Step [640/8569]\t loss_instance: 4.844795227050781\t loss_cluster: 3.2958383560180664\n",
      "Step [650/8569]\t loss_instance: 4.84686803817749\t loss_cluster: 3.2958409786224365\n",
      "Step [660/8569]\t loss_instance: 4.843237400054932\t loss_cluster: 3.29583477973938\n",
      "Step [670/8569]\t loss_instance: 4.844493865966797\t loss_cluster: 3.2958548069000244\n",
      "Step [680/8569]\t loss_instance: 4.844647407531738\t loss_cluster: 3.2958407402038574\n",
      "Step [690/8569]\t loss_instance: 4.84324312210083\t loss_cluster: 3.2958381175994873\n",
      "Step [700/8569]\t loss_instance: 4.843635082244873\t loss_cluster: 3.2958481311798096\n",
      "Step [710/8569]\t loss_instance: 4.842043399810791\t loss_cluster: 3.295830011367798\n",
      "Step [720/8569]\t loss_instance: 4.84313440322876\t loss_cluster: 3.295841932296753\n",
      "Step [730/8569]\t loss_instance: 4.842512607574463\t loss_cluster: 3.2958407402038574\n",
      "Step [740/8569]\t loss_instance: 4.8438873291015625\t loss_cluster: 3.29584002494812\n",
      "Step [750/8569]\t loss_instance: 4.8433756828308105\t loss_cluster: 3.2958433628082275\n",
      "Step [760/8569]\t loss_instance: 4.844295024871826\t loss_cluster: 3.2958359718322754\n",
      "Step [770/8569]\t loss_instance: 4.844095230102539\t loss_cluster: 3.2958359718322754\n",
      "Step [780/8569]\t loss_instance: 4.843863010406494\t loss_cluster: 3.2958381175994873\n",
      "Step [790/8569]\t loss_instance: 4.844143867492676\t loss_cluster: 3.295839786529541\n",
      "Step [800/8569]\t loss_instance: 4.844153881072998\t loss_cluster: 3.2958381175994873\n",
      "Step [810/8569]\t loss_instance: 4.844815254211426\t loss_cluster: 3.2958455085754395\n",
      "Step [820/8569]\t loss_instance: 4.8443684577941895\t loss_cluster: 3.295839309692383\n",
      "Step [830/8569]\t loss_instance: 4.844342231750488\t loss_cluster: 3.2958431243896484\n",
      "Step [840/8569]\t loss_instance: 4.84372615814209\t loss_cluster: 3.2958343029022217\n",
      "Step [850/8569]\t loss_instance: 4.843665599822998\t loss_cluster: 3.2958364486694336\n",
      "Step [860/8569]\t loss_instance: 4.844616889953613\t loss_cluster: 3.2958405017852783\n",
      "Step [870/8569]\t loss_instance: 4.844388484954834\t loss_cluster: 3.2958383560180664\n",
      "Step [880/8569]\t loss_instance: 4.844209671020508\t loss_cluster: 3.295841693878174\n",
      "Step [890/8569]\t loss_instance: 4.844883441925049\t loss_cluster: 3.2958390712738037\n",
      "Step [900/8569]\t loss_instance: 4.844182968139648\t loss_cluster: 3.295836925506592\n",
      "Step [910/8569]\t loss_instance: 4.844270706176758\t loss_cluster: 3.2958407402038574\n",
      "Step [920/8569]\t loss_instance: 4.844541549682617\t loss_cluster: 3.2958407402038574\n",
      "Step [930/8569]\t loss_instance: 4.844342231750488\t loss_cluster: 3.2958366870880127\n",
      "Step [940/8569]\t loss_instance: 4.843102931976318\t loss_cluster: 3.295830726623535\n",
      "Step [950/8569]\t loss_instance: 4.84413480758667\t loss_cluster: 3.2958414554595947\n",
      "Step [960/8569]\t loss_instance: 4.844141006469727\t loss_cluster: 3.2958412170410156\n",
      "Step [970/8569]\t loss_instance: 4.8440704345703125\t loss_cluster: 3.295840263366699\n",
      "Step [980/8569]\t loss_instance: 4.844122409820557\t loss_cluster: 3.2958412170410156\n",
      "Step [990/8569]\t loss_instance: 4.843989372253418\t loss_cluster: 3.2958388328552246\n",
      "Step [1000/8569]\t loss_instance: 4.8440728187561035\t loss_cluster: 3.295837163925171\n",
      "Step [1010/8569]\t loss_instance: 4.845202445983887\t loss_cluster: 3.2958409786224365\n",
      "Step [1020/8569]\t loss_instance: 4.845217227935791\t loss_cluster: 3.2958383560180664\n",
      "Step [1030/8569]\t loss_instance: 4.84444522857666\t loss_cluster: 3.2958414554595947\n",
      "Step [1040/8569]\t loss_instance: 4.8445725440979\t loss_cluster: 3.2958452701568604\n",
      "Step [1050/8569]\t loss_instance: 4.843990802764893\t loss_cluster: 3.295835018157959\n",
      "Step [1060/8569]\t loss_instance: 4.84505033493042\t loss_cluster: 3.2958428859710693\n",
      "Step [1070/8569]\t loss_instance: 4.843830585479736\t loss_cluster: 3.2958381175994873\n",
      "Step [1080/8569]\t loss_instance: 4.84396505355835\t loss_cluster: 3.2958428859710693\n",
      "Step [1090/8569]\t loss_instance: 4.843879222869873\t loss_cluster: 3.2958409786224365\n",
      "Step [1100/8569]\t loss_instance: 4.844427585601807\t loss_cluster: 3.2958436012268066\n",
      "Step [1110/8569]\t loss_instance: 4.844059944152832\t loss_cluster: 3.2958381175994873\n",
      "Step [1120/8569]\t loss_instance: 4.844561576843262\t loss_cluster: 3.295841693878174\n",
      "Step [1130/8569]\t loss_instance: 4.844359397888184\t loss_cluster: 3.295849323272705\n",
      "Step [1140/8569]\t loss_instance: 4.843743801116943\t loss_cluster: 3.295844078063965\n",
      "Step [1150/8569]\t loss_instance: 4.84423828125\t loss_cluster: 3.295840263366699\n",
      "Step [1160/8569]\t loss_instance: 4.843909740447998\t loss_cluster: 3.295837640762329\n",
      "Step [1170/8569]\t loss_instance: 4.84431266784668\t loss_cluster: 3.295839309692383\n",
      "Step [1180/8569]\t loss_instance: 4.843977451324463\t loss_cluster: 3.295844316482544\n",
      "Step [1190/8569]\t loss_instance: 4.8443522453308105\t loss_cluster: 3.295849084854126\n",
      "Step [1200/8569]\t loss_instance: 4.844620227813721\t loss_cluster: 3.295839309692383\n",
      "Step [1210/8569]\t loss_instance: 4.8442702293396\t loss_cluster: 3.2958438396453857\n",
      "Step [1220/8569]\t loss_instance: 4.843629837036133\t loss_cluster: 3.2958385944366455\n",
      "Step [1230/8569]\t loss_instance: 4.843938827514648\t loss_cluster: 3.295839548110962\n",
      "Step [1240/8569]\t loss_instance: 4.8440632820129395\t loss_cluster: 3.2958500385284424\n",
      "Step [1250/8569]\t loss_instance: 4.844438552856445\t loss_cluster: 3.2958450317382812\n",
      "Step [1260/8569]\t loss_instance: 4.844081878662109\t loss_cluster: 3.295837163925171\n",
      "Step [1270/8569]\t loss_instance: 4.84438419342041\t loss_cluster: 3.295839786529541\n",
      "Step [1280/8569]\t loss_instance: 4.844079971313477\t loss_cluster: 3.2958362102508545\n",
      "Step [1290/8569]\t loss_instance: 4.844284534454346\t loss_cluster: 3.2958383560180664\n",
      "Step [1300/8569]\t loss_instance: 4.844471454620361\t loss_cluster: 3.2958431243896484\n",
      "Step [1310/8569]\t loss_instance: 4.843785762786865\t loss_cluster: 3.2958388328552246\n",
      "Step [1320/8569]\t loss_instance: 4.844003200531006\t loss_cluster: 3.2958359718322754\n",
      "Step [1330/8569]\t loss_instance: 4.844789505004883\t loss_cluster: 3.295837640762329\n",
      "Step [1340/8569]\t loss_instance: 4.844272136688232\t loss_cluster: 3.295837879180908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\GitHub\\scCC\\scCC - ANN.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000010?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mstart_epoch, args\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000010?line=6'>7</a>\u001b[0m     lr \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000010?line=7'>8</a>\u001b[0m     loss_epoch \u001b[39m=\u001b[39m train(instance_loss,cluster_loss)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000010?line=8'>9</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss_epoch\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(scDataLoader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000010?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32md:\\Documents\\GitHub\\scCC\\scCC - ANN.ipynb Cell 10'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(instance_loss, cluster_loss)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000007?line=19'>20</a>\u001b[0m loss_cluster\u001b[39m=\u001b[39mcluster_loss(c_i,c_j)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000007?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_instance \u001b[39m+\u001b[39m loss_cluster\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000007?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000007?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/scCC/scCC%20-%20ANN.ipynb#ch0000007?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Tom-G/.conda/envs/torchenv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_device=torch.device('cuda')\n",
    "instance_loss=InstanceLoss(batch_size=args.batch_size,temperature=0.5,device=loss_device)\n",
    "cluster_loss=ClusterLoss(class_num=class_num,temperature=0.5,device=loss_device)\n",
    "accs=[]\n",
    "losses=[]\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    loss_epoch = train(instance_loss,cluster_loss)\n",
    "    losses.append(loss_epoch/len(scDataLoader))\n",
    "    if epoch % 10 == 0:\n",
    "        save_model(args, model, optimizer, epoch)\n",
    "    print(f\"\\nEpoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(scDataLoader)} \\n\")\n",
    "    nmi, ari, f, acc = test()\n",
    "    accs.append(acc)\n",
    "    print('Test NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    print('========'*8+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "ax[0].plot(accs)\n",
    "ax[0].set_title('Accuracy during Validation')\n",
    "ax[1].plot(losses)\n",
    "ax[1].set_title('Loss during Training')\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.suptitle('Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
