{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import os\n",
    "import anndata\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from modules import network,mlp\n",
    "from utils import yaml_config_hook,save_model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "config = yaml_config_hook(\"config/config.yaml\")\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "args = parser.parse_args([])\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "class_num = args.classnum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\anndata\\_core\\anndata.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if index_name in anno:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8569, 6000) (8569, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "          1    2    4       7        10   13   26   31        32   33  ...  \\\n0  1.024218  0.0  0.0  0.0000  1.302199  0.0  0.0  0.0  0.637877  0.0  ...   \n1  0.000000  0.0  0.0  0.0000  1.351171  0.0  0.0  0.0  0.000000  0.0  ...   \n2  0.000000  0.0  0.0  0.0000  0.000000  0.0  0.0  0.0  0.000000  0.0  ...   \n3  0.000000  0.0  0.0  0.4175  0.000000  0.0  0.0  0.0  0.000000  0.0  ...   \n4  0.000000  0.0  0.0  0.0000  0.000000  0.0  0.0  0.0  0.000000  0.0  ...   \n\n    20104  20105    20108  20109   20115  20118     20121   20122     20123  \\\n0  0.0000    0.0  0.36896    0.0  0.0000    0.0  0.637877  0.0000  0.000000   \n1  0.0000    0.0  0.00000    0.0  0.0000    0.0  0.888292  0.0000  0.305824   \n2  0.0000    0.0  0.00000    0.0  0.0000    0.0  0.000000  0.0000  0.000000   \n3  0.4175    0.0  0.00000    0.0  0.4175    0.0  0.937850  0.4175  0.000000   \n4  0.0000    0.0  0.00000    0.0  0.0000    0.0  0.509045  0.0000  0.000000   \n\n      20124  \n0  0.368960  \n1  0.000000  \n2  0.000000  \n3  0.000000  \n4  0.509045  \n\n[5 rows x 6000 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>4</th>\n      <th>7</th>\n      <th>10</th>\n      <th>13</th>\n      <th>26</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>...</th>\n      <th>20104</th>\n      <th>20105</th>\n      <th>20108</th>\n      <th>20109</th>\n      <th>20115</th>\n      <th>20118</th>\n      <th>20121</th>\n      <th>20122</th>\n      <th>20123</th>\n      <th>20124</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.024218</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>1.302199</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.637877</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.36896</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.637877</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.368960</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>1.351171</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.888292</td>\n      <td>0.0000</td>\n      <td>0.305824</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.4175</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.4175</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.4175</td>\n      <td>0.0</td>\n      <td>0.937850</td>\n      <td>0.4175</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.509045</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.509045</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 6000 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "sparse_X = scipy.sparse.load_npz('data/filtered_Counts.npz')\n",
    "annoData = pd.read_table('data/annoData.txt')\n",
    "y = annoData[\"cellIden\"].to_numpy()\n",
    "high_var_gene = 6000\n",
    "# normlization and feature selection\n",
    "adataSC = anndata.AnnData(X=sparse_X, obs=np.arange(sparse_X.shape[0]), var=np.arange(sparse_X.shape[1]))\n",
    "sc.pp.filter_genes(adataSC, min_cells=10)\n",
    "adataSC.raw = adataSC\n",
    "sc.pp.highly_variable_genes(adataSC, n_top_genes=high_var_gene, flavor='seurat_v3')\n",
    "sc.pp.normalize_total(adataSC, target_sum=1e4)\n",
    "sc.pp.log1p(adataSC)\n",
    "\n",
    "adataNorm = adataSC[:, adataSC.var.highly_variable]\n",
    "dataframe = adataNorm.to_df()\n",
    "x_ndarray = dataframe.values.squeeze()\n",
    "y_ndarray = np.expand_dims(y, axis=1)\n",
    "print(x_ndarray.shape,y_ndarray.shape)\n",
    "dataframe.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,random_split,TensorDataset\n",
    "\n",
    "scDataset = TensorDataset(torch.tensor(x_ndarray, dtype=torch.float32),\n",
    "                              torch.tensor(y_ndarray, dtype=torch.float32))\n",
    "\n",
    "scTrainLength = int(len(scDataset) * 0.8)\n",
    "scValidLength = len(scDataset) - scTrainLength\n",
    "scTrain, scValid = random_split(scDataset, [scTrainLength, scValidLength])\n",
    "\n",
    "scTrainDataLoader = DataLoader(scTrain, shuffle=True, batch_size=args.batch_size,drop_last=True)\n",
    "scValidDataLoader = DataLoader(scValid, shuffle=True, batch_size=args.batch_size,drop_last=True)\n",
    "\n",
    "for features, labels in scTrainDataLoader:\n",
    "    print(len(features[-1]))\n",
    "    print(len(features))\n",
    "    print(len(labels))\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# initialize model\n",
    "mlp = mlp.MLP(num_genes=args.num_genes)\n",
    "model = network.Network(mlp, args.feature_dim, args.classnum)\n",
    "model = model.to('cuda')\n",
    "# optimizer / loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class InstanceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,batch_size,temperature,device):\n",
    "        super(InstanceLoss,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.temperature=temperature\n",
    "        self.device=device\n",
    "\n",
    "        self.mask=self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "\n",
    "    def mask_correlated_samples(self,batch_size):\n",
    "        N=2*batch_size\n",
    "        mask=torch.ones(N,N)\n",
    "        mask=mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i,batch_size+1]=0\n",
    "            mask[batch_size+i,i]=0\n",
    "        mask=mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self,z_i,z_j):\n",
    "        N=2*self.batch_size\n",
    "        z=torch.cat((z_i,z_j),dim=0)\n",
    "\n",
    "        sim=torch.matmul(z,z.T)/self.temperature\n",
    "        sim_i_j=torch.diag(sim,self.batch_size)\n",
    "        sim_j_i=torch.diag(sim,-self.batch_size)\n",
    "\n",
    "        positive_samples= torch.cat((sim_i_j,sim_j_i),dim=0).reshape(N,1)\n",
    "        negative_samples=sim[self.mask].reshape(N,-1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class ClusterLoss(nn.Module):\n",
    "    def __init__(self, class_num, temperature, device):\n",
    "        super(ClusterLoss, self).__init__()\n",
    "        self.class_num = class_num\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "\n",
    "        self.mask = self.mask_correlated_clusters(class_num)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_clusters(self, class_num):\n",
    "        N = 2 * class_num\n",
    "        mask = torch.ones((N, N))\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(class_num):\n",
    "            mask[i, class_num + i] = 0\n",
    "            mask[class_num + i, i] = 0\n",
    "        mask = mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, c_i, c_j):\n",
    "        p_i = c_i.sum(0).view(-1)\n",
    "        p_i /= p_i.sum()\n",
    "        ne_i = math.log(p_i.size(0)) + (p_i * torch.log(p_i)).sum()\n",
    "        p_j = c_j.sum(0).view(-1)\n",
    "        p_j /= p_j.sum()\n",
    "        ne_j = math.log(p_j.size(0)) + (p_j * torch.log(p_j)).sum()\n",
    "        ne_loss = ne_i + ne_j\n",
    "\n",
    "        c_i = c_i.t()\n",
    "        c_j = c_j.t()\n",
    "        N = 2 * self.class_num\n",
    "        c = torch.cat((c_i, c_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(c.unsqueeze(1), c.unsqueeze(0)) / self.temperature\n",
    "        sim_i_j = torch.diag(sim, self.class_num)\n",
    "        sim_j_i = torch.diag(sim, -self.class_num)\n",
    "\n",
    "        positive_clusters = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_clusters = sim[self.mask].reshape(N, -1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_clusters.device).long()\n",
    "        logits = torch.cat((positive_clusters, negative_clusters), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss + ne_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# loss_device=torch.device('cuda')\n",
    "# instance_loss=InstanceLoss(batch_size=args.batch_size,temperature=0.5,device=loss_device)\n",
    "# cluster_loss=ClusterLoss(class_num=class_num,temperature=0.5,device=loss_device)\n",
    "# loss_epoch = 0\n",
    "# for step, (data,label) in enumerate(scTrainDataLoader):\n",
    "#     optimizer.zero_grad()\n",
    "#     x_i=data.clone().to('cuda')\n",
    "#     x_j=data.clone().to('cuda')\n",
    "#     z_i,z_j,c_i,c_j=model(x_i,x_j)\n",
    "#     loss_instance=instance_loss(z_i,z_j)\n",
    "#     loss_cluster=cluster_loss(c_i,c_j)\n",
    "#     loss = loss_instance + loss_cluster\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if step % 10 == 0:\n",
    "#         print(\n",
    "#             f\"Step [{step}/{len(scTrainDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n",
    "#     loss_epoch += loss.item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def train(instance_loss,cluster_loss,device):\n",
    "    loss_epoch = 0\n",
    "    for step, (data,label) in enumerate(scTrainDataLoader):\n",
    "        optimizer.zero_grad()\n",
    "        x_i=data.clone().to('cuda')\n",
    "        x_j=data.clone().to('cuda')\n",
    "        z_i,z_j,c_i,c_j=model(x_i,x_j)\n",
    "        loss_instance=instance_loss(z_i,z_j)\n",
    "        loss_cluster=cluster_loss(c_i,c_j)\n",
    "        loss = loss_instance + loss_cluster\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                f\"Step [{step}/{len(scTrainDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def inference(loader, model, device):\n",
    "    model.eval()\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            c = model.forward_cluster(x)\n",
    "        c = c.detach()\n",
    "        feature_vector.extend(c.cpu().detach().numpy())\n",
    "        labels_vector.extend(y.numpy())\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    # print(feature_vector.shape, labels_vector.shape)\n",
    "    return feature_vector, labels_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from evaluation import evaluation\n",
    "def test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X, Y = inference(scValidDataLoader, model, device)\n",
    "    print(X.shape,Y.shape)\n",
    "    nmi, ari, f, acc = evaluation.evaluate(Y.reshape(-1), X)\n",
    "    # print('NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    return nmi, ari, f, acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/26]\t loss_instance: 6.231241226196289\t loss_cluster: 3.2991526126861572\n",
      "Step [10/26]\t loss_instance: 6.228573322296143\t loss_cluster: 3.2989978790283203\n",
      "Step [20/26]\t loss_instance: 6.226306915283203\t loss_cluster: 3.298896551132202\n",
      "\n",
      "Epoch [0/100]\t Loss: 9.526805070730356 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.1277 ARI = 0.0626 F = 0.2868 ACC = 0.0703\n",
      "================================================================\n",
      "\n",
      "Step [0/26]\t loss_instance: 6.2114410400390625\t loss_cluster: 3.2988059520721436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/26]\t loss_instance: 6.184502124786377\t loss_cluster: 3.298677682876587\n",
      "Step [20/26]\t loss_instance: 6.115828990936279\t loss_cluster: 3.298475742340088\n",
      "\n",
      "Epoch [1/100]\t Loss: 9.454911818871132 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.1104 ARI = -0.0026 F = 0.3360 ACC = 0.0475\n",
      "================================================================\n",
      "\n",
      "Step [0/26]\t loss_instance: 6.0420122146606445\t loss_cluster: 3.2982559204101562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/26]\t loss_instance: 5.9172821044921875\t loss_cluster: 3.29764461517334\n",
      "Step [20/26]\t loss_instance: 5.855574607849121\t loss_cluster: 3.296943426132202\n",
      "\n",
      "Epoch [2/100]\t Loss: 9.193093593303974 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.3755 ARI = 0.2985 F = 0.4635 ACC = 0.0267\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/26]\t loss_instance: 5.743744850158691\t loss_cluster: 3.295684576034546\n",
      "Step [10/26]\t loss_instance: 5.679439067840576\t loss_cluster: 3.294421434402466\n",
      "Step [20/26]\t loss_instance: 5.657205581665039\t loss_cluster: 3.293341636657715\n",
      "\n",
      "Epoch [3/100]\t Loss: 8.968069406656118 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.4789 ARI = 0.4235 F = 0.5414 ACC = 0.0469\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/26]\t loss_instance: 5.621006488800049\t loss_cluster: 3.2922980785369873\n",
      "Step [10/26]\t loss_instance: 5.551555156707764\t loss_cluster: 3.2902541160583496\n",
      "Step [20/26]\t loss_instance: 5.471215724945068\t loss_cluster: 3.2890233993530273\n",
      "\n",
      "Epoch [4/100]\t Loss: 8.815011134514442 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.4947 ARI = 0.3478 F = 0.4704 ACC = 0.0892\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/26]\t loss_instance: 5.4052934646606445\t loss_cluster: 3.288037061691284\n",
      "Step [10/26]\t loss_instance: 5.3280110359191895\t loss_cluster: 3.2848637104034424\n",
      "Step [20/26]\t loss_instance: 5.249216079711914\t loss_cluster: 3.2842748165130615\n",
      "\n",
      "Epoch [5/100]\t Loss: 8.601975807776817 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.5582 ARI = 0.4657 F = 0.5670 ACC = 0.0755\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/26]\t loss_instance: 5.22073221206665\t loss_cluster: 3.2817556858062744\n",
      "Step [10/26]\t loss_instance: 5.166556358337402\t loss_cluster: 3.2804930210113525\n",
      "Step [20/26]\t loss_instance: 5.138511657714844\t loss_cluster: 3.2769439220428467\n",
      "\n",
      "Epoch [6/100]\t Loss: 8.44427673633282 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.5878 ARI = 0.5222 F = 0.6110 ACC = 0.6530\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/26]\t loss_instance: 5.108696937561035\t loss_cluster: 3.274916648864746\n",
      "Step [10/26]\t loss_instance: 5.083678722381592\t loss_cluster: 3.2718863487243652\n",
      "Step [20/26]\t loss_instance: 5.049376487731934\t loss_cluster: 3.2654192447662354\n",
      "\n",
      "Epoch [7/100]\t Loss: 8.34597264803373 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.5595 ARI = 0.4840 F = 0.5792 ACC = 0.6296\n",
      "================================================================\n",
      "\n",
      "Step [0/26]\t loss_instance: 5.030228614807129\t loss_cluster: 3.263798475265503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/26]\t loss_instance: 5.007358551025391\t loss_cluster: 3.2585785388946533\n",
      "Step [20/26]\t loss_instance: 4.972696304321289\t loss_cluster: 3.2534735202789307\n",
      "\n",
      "Epoch [8/100]\t Loss: 8.260188542879545 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.5024 ARI = 0.3906 F = 0.5018 ACC = 0.5534\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/26]\t loss_instance: 4.9634623527526855\t loss_cluster: 3.2483842372894287\n",
      "Step [10/26]\t loss_instance: 4.932798385620117\t loss_cluster: 3.2410998344421387\n",
      "Step [20/26]\t loss_instance: 4.906030654907227\t loss_cluster: 3.23221755027771\n",
      "\n",
      "Epoch [9/100]\t Loss: 8.165750320141132 \n",
      "\n",
      "Step [0/6]\t Computing features...\n",
      "Features shape (1536,)\n",
      "(1536,) (1536, 1)\n",
      "Test NMI = 0.5057 ARI = 0.4347 F = 0.5453 ACC = 0.6100\n",
      "================================================================\n",
      "\n",
      "Step [0/26]\t loss_instance: 4.877933502197266\t loss_cluster: 3.2284905910491943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/26]\t loss_instance: 4.8615498542785645\t loss_cluster: 3.219658851623535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14936/3061959597.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mlr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparam_groups\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"lr\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mloss_epoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minstance_loss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcluster_loss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_device\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m10\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[0msave_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14936/296819680.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(instance_loss, cluster_loss, device)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minstance_loss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcluster_loss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mloss_epoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlabel\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscTrainDataLoader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[0mx_i\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'cuda'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    520\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 521\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    522\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    523\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    560\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 561\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    562\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    563\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollate_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001B[0m in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     82\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'each element in list of batch should be of equal size'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[0mtransposed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 84\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mdefault_collate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msamples\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtransposed\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     85\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m     \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdefault_collate_err_msg_format\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melem_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     82\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'each element in list of batch should be of equal size'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[0mtransposed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 84\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mdefault_collate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msamples\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtransposed\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     85\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m     \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdefault_collate_err_msg_format\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melem_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Environments\\MiniConda\\envs\\torchenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001B[0m in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     54\u001B[0m             \u001B[0mstorage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0melem\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstorage\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_new_shared\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnumel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0melem\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnew\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstorage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0melem_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__module__\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'numpy'\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0melem_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m'str_'\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0melem_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m'string_'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_device=torch.device('cuda')\n",
    "instance_loss=InstanceLoss(batch_size=args.batch_size,temperature=0.5,device=loss_device)\n",
    "cluster_loss=ClusterLoss(class_num=class_num,temperature=0.5,device=loss_device)\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    loss_epoch = train(instance_loss,cluster_loss,loss_device)\n",
    "    if epoch % 10 == 0:\n",
    "        save_model(args, model, optimizer, epoch)\n",
    "    print(f\"\\nEpoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(scTrainDataLoader)} \\n\")\n",
    "    nmi, ari, f, acc = test()\n",
    "    print('Test NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    print('========'*8+'\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}