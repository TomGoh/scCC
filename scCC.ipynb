{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import os\n",
    "import anndata\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from modules import network,mlp\n",
    "from utils import yaml_config_hook,save_model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "config = yaml_config_hook(\"config/config.yaml\")\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "args = parser.parse_args([])\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed_all(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "# np.random.seed(args.seed)\n",
    "class_num = args.classnum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\anndata\\_core\\anndata.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if index_name in anno:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8569, 6000) (8569, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "          1    2    4       7        10   13   26   31        32   33  ...  \\\n0  1.024218  0.0  0.0  0.0000  1.302199  0.0  0.0  0.0  0.637877  0.0  ...   \n1  0.000000  0.0  0.0  0.0000  1.351171  0.0  0.0  0.0  0.000000  0.0  ...   \n2  0.000000  0.0  0.0  0.0000  0.000000  0.0  0.0  0.0  0.000000  0.0  ...   \n3  0.000000  0.0  0.0  0.4175  0.000000  0.0  0.0  0.0  0.000000  0.0  ...   \n4  0.000000  0.0  0.0  0.0000  0.000000  0.0  0.0  0.0  0.000000  0.0  ...   \n\n    20104  20105    20108  20109   20115  20118     20121   20122     20123  \\\n0  0.0000    0.0  0.36896    0.0  0.0000    0.0  0.637877  0.0000  0.000000   \n1  0.0000    0.0  0.00000    0.0  0.0000    0.0  0.888292  0.0000  0.305824   \n2  0.0000    0.0  0.00000    0.0  0.0000    0.0  0.000000  0.0000  0.000000   \n3  0.4175    0.0  0.00000    0.0  0.4175    0.0  0.937850  0.4175  0.000000   \n4  0.0000    0.0  0.00000    0.0  0.0000    0.0  0.509045  0.0000  0.000000   \n\n      20124  \n0  0.368960  \n1  0.000000  \n2  0.000000  \n3  0.000000  \n4  0.509045  \n\n[5 rows x 6000 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>4</th>\n      <th>7</th>\n      <th>10</th>\n      <th>13</th>\n      <th>26</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>...</th>\n      <th>20104</th>\n      <th>20105</th>\n      <th>20108</th>\n      <th>20109</th>\n      <th>20115</th>\n      <th>20118</th>\n      <th>20121</th>\n      <th>20122</th>\n      <th>20123</th>\n      <th>20124</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.024218</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>1.302199</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.637877</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.36896</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.637877</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.368960</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>1.351171</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.888292</td>\n      <td>0.0000</td>\n      <td>0.305824</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.4175</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.4175</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.4175</td>\n      <td>0.0</td>\n      <td>0.937850</td>\n      <td>0.4175</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.509045</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.509045</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 6000 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "sparse_X = scipy.sparse.load_npz('data/filtered_Counts.npz')\n",
    "annoData = pd.read_table('data/annoData.txt')\n",
    "y = annoData[\"cellIden\"].to_numpy()\n",
    "high_var_gene = args.num_genes\n",
    "# normlization and feature selection\n",
    "adataSC = anndata.AnnData(X=sparse_X, obs=np.arange(sparse_X.shape[0]), var=np.arange(sparse_X.shape[1]))\n",
    "sc.pp.filter_genes(adataSC, min_cells=10)\n",
    "adataSC.raw = adataSC\n",
    "sc.pp.highly_variable_genes(adataSC, n_top_genes=high_var_gene, flavor='seurat_v3')\n",
    "sc.pp.normalize_total(adataSC, target_sum=1e4)\n",
    "sc.pp.log1p(adataSC)\n",
    "\n",
    "adataNorm = adataSC[:, adataSC.var.highly_variable]\n",
    "dataframe = adataNorm.to_df()\n",
    "x_ndarray = dataframe.values.squeeze()\n",
    "y_ndarray = np.expand_dims(y, axis=1)\n",
    "print(x_ndarray.shape,y_ndarray.shape)\n",
    "dataframe.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,random_split,TensorDataset\n",
    "scDataset = TensorDataset(torch.tensor(x_ndarray, dtype=torch.float32),\n",
    "                              torch.tensor(y_ndarray, dtype=torch.float32))\n",
    "\n",
    "scTrainLength = int(len(scDataset) * 0.8)\n",
    "scValidLength = len(scDataset) - scTrainLength\n",
    "scTrain, scValid = random_split(scDataset, [scTrainLength, scValidLength])\n",
    "# print(scValidLength)\n",
    "scTrainDataLoader = DataLoader(scTrain, shuffle=True, batch_size=args.batch_size,drop_last=True)\n",
    "scValidDataLoader = DataLoader(scValid, shuffle=False, batch_size=args.batch_size,drop_last=False)\n",
    "\n",
    "for features, labels in scTrainDataLoader:\n",
    "    print(len(features[-1]))\n",
    "    print(len(features))\n",
    "    print(len(labels))\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# initialize model\n",
    "mlp = mlp.MLP(num_genes=args.num_genes)\n",
    "model = network.Network(mlp, args.feature_dim, args.classnum)\n",
    "model = model.to('cuda')\n",
    "# optimizer / loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class InstanceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,batch_size,temperature,device):\n",
    "        super(InstanceLoss,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.temperature=temperature\n",
    "        self.device=device\n",
    "\n",
    "        self.mask=self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "\n",
    "    def mask_correlated_samples(self,batch_size):\n",
    "        N=2*batch_size\n",
    "        mask=torch.ones(N,N)\n",
    "        mask=mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i,batch_size+1]=0\n",
    "            mask[batch_size+i,i]=0\n",
    "        mask=mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self,z_i,z_j):\n",
    "        N=2*self.batch_size\n",
    "        z=torch.cat((z_i,z_j),dim=0)\n",
    "\n",
    "        sim=torch.matmul(z,z.T)/self.temperature\n",
    "        sim_i_j=torch.diag(sim,self.batch_size)\n",
    "        sim_j_i=torch.diag(sim,-self.batch_size)\n",
    "\n",
    "        positive_samples= torch.cat((sim_i_j,sim_j_i),dim=0).reshape(N,1)\n",
    "        negative_samples=sim[self.mask].reshape(N,-1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ClusterLoss(nn.Module):\n",
    "    def __init__(self, class_num, temperature, device):\n",
    "        super(ClusterLoss, self).__init__()\n",
    "        self.class_num = class_num\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "\n",
    "        self.mask = self.mask_correlated_clusters(class_num)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_clusters(self, class_num):\n",
    "        N = 2 * class_num\n",
    "        mask = torch.ones((N, N))\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(class_num):\n",
    "            mask[i, class_num + i] = 0\n",
    "            mask[class_num + i, i] = 0\n",
    "        mask = mask.bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, c_i, c_j):\n",
    "        p_i = c_i.sum(0).view(-1)\n",
    "        p_i /= p_i.sum()\n",
    "        ne_i = math.log(p_i.size(0)) + (p_i * torch.log(p_i)).sum()\n",
    "        p_j = c_j.sum(0).view(-1)\n",
    "        p_j /= p_j.sum()\n",
    "        ne_j = math.log(p_j.size(0)) + (p_j * torch.log(p_j)).sum()\n",
    "        ne_loss = ne_i + ne_j\n",
    "\n",
    "        c_i = c_i.t()\n",
    "        c_j = c_j.t()\n",
    "        N = 2 * self.class_num\n",
    "        c = torch.cat((c_i, c_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(c.unsqueeze(1), c.unsqueeze(0)) / self.temperature\n",
    "        sim_i_j = torch.diag(sim, self.class_num)\n",
    "        sim_j_i = torch.diag(sim, -self.class_num)\n",
    "\n",
    "        positive_clusters = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_clusters = sim[self.mask].reshape(N, -1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_clusters.device).long()\n",
    "        logits = torch.cat((positive_clusters, negative_clusters), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss + ne_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from preprocessing import transformation\n",
    "def train(instance_loss,cluster_loss):\n",
    "    loss_epoch = 0\n",
    "    for step, (data,label) in enumerate(scTrainDataLoader):\n",
    "        optimizer.zero_grad()\n",
    "        x_i=data.clone()\n",
    "        x_j=transformation.transformation(x_i)\n",
    "        if x_j is not None:\n",
    "            x_j=x_j.to('cuda')\n",
    "        else:\n",
    "            x_j=x_i.clone().to('cuda')\n",
    "        x_i=x_i.to('cuda')\n",
    "        # print('x_j '+str(x_j.device))\n",
    "        # print('x_i '+str(x_i.device))\n",
    "        # x_i=x_i.to('cuda')\n",
    "        # x_j=x_i.clone().to('cuda')\n",
    "        z_i,z_j,c_i,c_j=model(x_i,x_j)\n",
    "        loss_instance=instance_loss(z_i,z_j)\n",
    "        loss_cluster=cluster_loss(c_i,c_j)\n",
    "        loss = loss_instance + loss_cluster\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                f\"Step [{step}/{len(scTrainDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def inference(loader, model, device):\n",
    "    model.eval()\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            c = model.forward_cluster(x)\n",
    "        c = c.detach()\n",
    "        feature_vector.extend(c.cpu().detach().numpy())\n",
    "        labels_vector.extend(y.numpy())\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    # print(feature_vector.shape, labels_vector.shape)\n",
    "    return feature_vector, labels_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from evaluation import evaluation\n",
    "def test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X, Y = inference(scValidDataLoader, model, device)\n",
    "    # print(X.shape,Y.shape)\n",
    "    nmi, ari, f, acc = evaluation.evaluate(Y.reshape(-1), X)\n",
    "    # print('NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    return nmi, ari, f, acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/6]\t loss_instance: 7.623985767364502\t loss_cluster: 3.2990169525146484\n",
      "\n",
      "Epoch [0/100]\t Loss: 10.923038005828857 \n",
      "\n",
      "Step [0/1]\t Computing features...\n",
      "Features shape (1024,)\n",
      "13\n",
      "Test NMI = 0.0000 ARI = 0.0000 F = 0.4483 ACC = 0.0000\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/6]\t loss_instance: 7.624078750610352\t loss_cluster: 3.29891037940979\n",
      "\n",
      "Epoch [1/100]\t Loss: 10.922920227050781 \n",
      "\n",
      "Step [0/1]\t Computing features...\n",
      "Features shape (1024,)\n",
      "13\n",
      "Test NMI = 0.0000 ARI = 0.0000 F = 0.4483 ACC = 0.0000\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/6]\t loss_instance: 7.623790264129639\t loss_cluster: 3.2988064289093018\n",
      "\n",
      "Epoch [2/100]\t Loss: 10.922549883524576 \n",
      "\n",
      "Step [0/1]\t Computing features...\n",
      "Features shape (1024,)\n",
      "13\n",
      "Test NMI = 0.0000 ARI = 0.0000 F = 0.4483 ACC = 0.0000\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/6]\t loss_instance: 7.623844146728516\t loss_cluster: 3.2987308502197266\n",
      "\n",
      "Epoch [3/100]\t Loss: 10.922451814015707 \n",
      "\n",
      "Step [0/1]\t Computing features...\n",
      "Features shape (1024,)\n",
      "13\n",
      "Test NMI = 0.0000 ARI = 0.0000 F = 0.4483 ACC = 0.0000\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/6]\t loss_instance: 7.6235809326171875\t loss_cluster: 3.2986605167388916\n",
      "\n",
      "Epoch [4/100]\t Loss: 10.92166837056478 \n",
      "\n",
      "Step [0/1]\t Computing features...\n",
      "Features shape (1024,)\n",
      "13\n",
      "Test NMI = 0.0000 ARI = 0.0000 F = 0.4483 ACC = 0.0000\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/6]\t loss_instance: 7.623159885406494\t loss_cluster: 3.298602342605591\n",
      "\n",
      "Epoch [5/100]\t Loss: 10.920653184254965 \n",
      "\n",
      "Step [0/1]\t Computing features...\n",
      "Features shape (1024,)\n",
      "13\n",
      "Test NMI = 0.0000 ARI = 0.0000 F = 0.4483 ACC = 0.0000\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Environment\\Conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/6]\t loss_instance: 7.622228145599365\t loss_cluster: 3.2985475063323975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12576/2730613791.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mlr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparam_groups\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"lr\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mloss_epoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minstance_loss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcluster_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m     \u001B[0mlosses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss_epoch\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscTrainDataLoader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m10\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12576/583639533.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(instance_loss, cluster_loss)\u001B[0m\n\u001B[0;32m     24\u001B[0m             print(\n\u001B[0;32m     25\u001B[0m                 f\"Step [{step}/{len(scTrainDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n\u001B[1;32m---> 26\u001B[1;33m         \u001B[0mloss_epoch\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mloss_epoch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_device=torch.device('cuda')\n",
    "instance_loss=InstanceLoss(batch_size=args.batch_size,temperature=0.5,device=loss_device)\n",
    "cluster_loss=ClusterLoss(class_num=class_num,temperature=0.5,device=loss_device)\n",
    "accs=[]\n",
    "losses=[]\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    loss_epoch = train(instance_loss,cluster_loss)\n",
    "    losses.append(loss_epoch/len(scTrainDataLoader))\n",
    "    if epoch % 10 == 0:\n",
    "        save_model(args, model, optimizer, epoch)\n",
    "    print(f\"\\nEpoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(scTrainDataLoader)} \\n\")\n",
    "    nmi, ari, f, acc = test()\n",
    "    accs.append(acc)\n",
    "    print('Test NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    print('========'*8+'\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "ax[0].plot(accs)\n",
    "ax[0].set_title('Accuracy during Validation')\n",
    "ax[1].plot(losses)\n",
    "ax[1].set_title('Loss during Training')\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.suptitle('Result')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}