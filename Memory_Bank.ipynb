{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import os\n",
    "import anndata\n",
    "import math\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import network,mlp,contrastive_loss\n",
    "from utils import yaml_config_hook\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "config = yaml_config_hook(\"config/config.yaml\")\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "args = parser.parse_args([])\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed_all(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "# np.random.seed(args.seed)\n",
    "class_num = args.classnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\anndata\\_core\\anndata.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if index_name in anno:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8569, 2000) (8569, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>41</th>\n",
       "      <th>45</th>\n",
       "      <th>62</th>\n",
       "      <th>68</th>\n",
       "      <th>106</th>\n",
       "      <th>133</th>\n",
       "      <th>147</th>\n",
       "      <th>...</th>\n",
       "      <th>19763</th>\n",
       "      <th>19786</th>\n",
       "      <th>19808</th>\n",
       "      <th>19854</th>\n",
       "      <th>19883</th>\n",
       "      <th>20021</th>\n",
       "      <th>20073</th>\n",
       "      <th>20109</th>\n",
       "      <th>20121</th>\n",
       "      <th>20124</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.302199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.637877</td>\n",
       "      <td>0.368960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.351171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.509045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     2        10   13   41   45        62       68  106  133  147  ...  19763  \\\n",
       "0  0.0  1.302199  0.0  0.0  0.0  0.000000  0.36896  0.0  0.0  0.0  ...    0.0   \n",
       "1  0.0  1.351171  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "2  0.0  0.000000  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "3  0.0  0.000000  0.0  0.0  0.0  0.711146  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "4  0.0  0.000000  0.0  0.0  0.0  0.509045  0.00000  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "      19786  19808  19854    19883  20021  20073  20109     20121     20124  \n",
       "0  0.000000    0.0    0.0  0.00000    0.0    0.0    0.0  0.637877  0.368960  \n",
       "1  0.000000    0.0    0.0  0.00000    0.0    0.0    0.0  0.888292  0.000000  \n",
       "2  0.000000    0.0    0.0  0.00000    0.0    0.0    0.0  0.000000  0.000000  \n",
       "3  0.417500    0.0    0.0  0.93785    0.0    0.0    0.0  0.937850  0.000000  \n",
       "4  0.509045    0.0    0.0  0.00000    0.0    0.0    0.0  0.509045  0.509045  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "sparse_X = scipy.sparse.load_npz('data/filtered_Counts.npz')\n",
    "annoData = pd.read_table('data/annoData.txt')\n",
    "y = annoData[\"cellIden\"].to_numpy()\n",
    "high_var_gene = args.num_genes\n",
    "# normlization and feature selection\n",
    "adataSC = anndata.AnnData(X=sparse_X, obs=np.arange(sparse_X.shape[0]), var=np.arange(sparse_X.shape[1]))\n",
    "sc.pp.filter_genes(adataSC, min_cells=10)\n",
    "adataSC.raw = adataSC\n",
    "sc.pp.highly_variable_genes(adataSC, n_top_genes=high_var_gene, flavor='seurat_v3')\n",
    "sc.pp.normalize_total(adataSC, target_sum=1e4)\n",
    "sc.pp.log1p(adataSC)\n",
    "\n",
    "adataNorm = adataSC[:, adataSC.var.highly_variable]\n",
    "dataframe = adataNorm.to_df()\n",
    "x_ndarray = dataframe.values.squeeze()\n",
    "y_ndarray = np.expand_dims(y, axis=1)\n",
    "print(x_ndarray.shape,y_ndarray.shape)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = mlp.MLP(num_genes=args.num_genes)\n",
    "model = network.Network(mlp, args.feature_dim, args.classnum)\n",
    "model = model.to('cuda')\n",
    "# optimizer / loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "scDataset = TensorDataset(torch.tensor(x_ndarray, dtype=torch.float32),\n",
    "                              torch.tensor(y_ndarray, dtype=torch.float32))\n",
    "\n",
    "scDataLoader = DataLoader(scDataset, shuffle=True, batch_size=1024,drop_last=True)\n",
    "\n",
    "for features, labels in scDataLoader:\n",
    "    print(len(features[-1]))\n",
    "    print(len(features))\n",
    "    print(len(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MemoryBank\n",
    "\n",
    "首先初始化时传入各种参数数值并建立一个基于原始数据的hnsw的Bank\n",
    "\n",
    "而后每一次调用该对象时，传入一个batch的数据，并以矩阵的形式返回该batch的每个样本的Top 10近邻\n",
    "\n",
    "而每次需要结合Embedding进行更新bank时使用update函数重新建立索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank():\n",
    "\n",
    "    # 初始化，传入参数\n",
    "    def __init__(self,batch_size,full_data,topK=10):\n",
    "        self.topK=topK\n",
    "        self.batch_size=batch_size\n",
    "        self.bank=None\n",
    "        self.full_data=full_data\n",
    "\n",
    "    # 根据在updateBank中更新的hnsw对象以及输入的数据data（这里可以是embedding）提取TopK个近邻的数据\n",
    "    # 返回的结果是一个形状为[TopK,batch_size,num_genes]的数组，从第一个维度来看，\n",
    "    # 每个[batch_size,num_genes]的子数组都是根据输入的数据data寻找的一个近邻，一共TopK个\n",
    "    def generateContrast(self,data):\n",
    "        if self.bank is not None:\n",
    "            contrasts=np.empty((self.topK,args.batch_size,args.num_genes))\n",
    "            labels,distances=self.bank.knn_query(data,k=self.topK)\n",
    "            \n",
    "            print(labels)\n",
    "\n",
    "            for step,label in enumerate(labels):\n",
    "                contrasts[:,step]=self.full_data[label.tolist()]\n",
    "            return contrasts\n",
    "        else:\n",
    "            print('Memory Bank has not been initialized......')\n",
    "            raise NotImplementedError()\n",
    "            return None\n",
    "\n",
    "    # 根据输入的embedding更新hnsw对象\n",
    "    def updateBank(self,embedding):\n",
    "        num_elements=len(embedding)\n",
    "        dim=embedding.shape[1]\n",
    "        self.bank=hnswlib.Index(space='cosine',dim=dim)\n",
    "        self.bank.init_index(max_elements=num_elements, ef_construction=100, M=16)\n",
    "        self.bank.set_ef(100)\n",
    "        self.bank.set_num_threads(4)\n",
    "        self.bank.add_items(embedding)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Model中的MLP生成整个数据集的embedding\n",
    "\n",
    "需要另外新建一个不打乱数据顺序的数据集来往Model中传参数\n",
    "\n",
    "并且需要注意在迭代更新Memory Bank后必须使用经过model处理的样本来进行索引查询，否则差距巨大！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 128)\n",
      "[[   0 1202 1197 1207  573 1216    1 1221   46 6371]]\n",
      "(10, 1024, 2000)\n",
      "[[   0 3834 4771 1197 1207  172  706 5455 1221 1203]]\n",
      "(10, 1024, 2000)\n"
     ]
    }
   ],
   "source": [
    "embeddings=np.empty((0,mlp.rep_dim))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for step, (x, y) in enumerate(scDataLoader):\n",
    "        x=x.to('cuda')\n",
    "        embedding=model.forward_embedding(x).cpu().detach().numpy()                 \n",
    "        embeddings=np.row_stack((embeddings,embedding))\n",
    "# for embedding in embeddings:\n",
    "#     print(embedding)\n",
    "#         # Model输出的embedding的形状是batch_size x mlp.rep_dim\n",
    "print(embeddings.shape)\n",
    "test_bank=MemoryBank(batch_size=args.batch_size,full_data=x_ndarray,topK=10)\n",
    "test_bank.updateBank(x_ndarray)\n",
    "test_result_1=test_bank.generateContrast(x_ndarray[0])\n",
    "print(test_result_1.shape)\n",
    "test_bank.updateBank(embedding=embeddings)\n",
    "\n",
    "# 查询时必须经过model的mlp传输生成embedding才可查询\n",
    "test_x_0=model.forward_embedding(torch.tensor(x_ndarray[0],dtype=torch.float32).to('cuda')).cpu().detach().numpy()  \n",
    "test_result_2=test_bank.generateContrast(test_x_0)\n",
    "print(test_result_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1024, 2000)\n",
      "(10, 1024, 2000)\n",
      "(10, 1024, 2000)\n",
      "(10, 1024, 2000)\n",
      "(10, 1024, 2000)\n",
      "(10, 1024, 2000)\n",
      "(10, 1024, 2000)\n",
      "(10, 1024, 2000)\n"
     ]
    }
   ],
   "source": [
    "# memoryBank=MemoryBank(args.batch_size,x_ndarray,topK=10)\n",
    "# memoryBank.updateBank(x_ndarray)\n",
    "# for featres,tags in scDataLoader:\n",
    "#     result=memoryBank.generateContrast(featres.numpy())\n",
    "#     print(result.shape)\n",
    "#     # print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_bank=MemoryBank(batch_size=args.batch_size,full_data=x_ndarray,topK=10)\n",
    "instance_loss=contrastive_loss.InstanceLoss(batch_size=args.batch_size,temperature=0.5,device='cuda')\n",
    "cluster_loss=contrastive_loss.ClusterLoss(class_num=class_num,temperature=0.5,device='cuda')\n",
    "memory_bank.updateBank(x_ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数一次会针对一个batch中的所有样本生成一个形状为 [TopK,batch_size,num_genes] 的矩阵，分别对应原有batch中的每一个样本的TopK近邻。\n",
    "\n",
    "而后针对这一个针对所有batch内样本索引TopK近邻的对比矩阵分别进行TopK次对比学习，对于batch内的每一个样本而言，每一次均是使用一个近邻构成正样本对、以及其余的batch_size*2-2的样本构成负样本对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom-G\\.conda\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ loss:\n",
      "10.92076587677002\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92131233215332\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921283721923828\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921366691589355\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921391487121582\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921318054199219\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921403884887695\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921327590942383\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921369552612305\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92131233215332\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.920676231384277\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921058654785156\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921152114868164\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921211242675781\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921113014221191\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921199798583984\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921171188354492\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921113967895508\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921247482299805\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921133995056152\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.920716285705566\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921144485473633\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.9213285446167\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921168327331543\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92124080657959\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921279907226562\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921330451965332\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921273231506348\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921297073364258\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921308517456055\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.920766830444336\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921229362487793\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92115592956543\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921307563781738\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921208381652832\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921366691589355\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921271324157715\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921323776245117\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921308517456055\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92122745513916\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.920724868774414\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921268463134766\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92128849029541\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92129135131836\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921403884887695\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921309471130371\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921339988708496\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921329498291016\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921335220336914\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921285629272461\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.920729637145996\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921262741088867\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921411514282227\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921293258666992\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921321868896484\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921321868896484\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921222686767578\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92134952545166\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921412467956543\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921293258666992\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.920770645141602\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92125129699707\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92119026184082\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92140007019043\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92131519317627\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921343803405762\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921313285827637\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921399116516113\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92133903503418\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.92131519317627\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.920772552490234\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921417236328125\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921343803405762\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921350479125977\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921472549438477\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921393394470215\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921355247497559\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921329498291016\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921409606933594\n",
      "sample shape:\n",
      "(1024, 2000)\n",
      "data shape:\n",
      "torch.Size([1024, 2000])\n",
      "------ loss:\n",
      "10.921375274658203\n"
     ]
    }
   ],
   "source": [
    "def train(instance_loss,cluster_loss,memory_bank):\n",
    "    loss_epoch=0\n",
    "    for step,(data,label) in enumerate(scDataLoader):\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        contrast_samples=memory_bank.generateContrast(data)\n",
    "        iter_times=contrast_samples.shape[0]\n",
    "        for sample in contrast_samples:\n",
    "            print(f'sample shape:\\n{sample.shape}')\n",
    "            print(f'data shape:\\n{data.shape}')\n",
    "            x_i=data.clone().to('cuda')\n",
    "            x_j=torch.tensor(sample,dtype=torch.float32).to('cuda')\n",
    "\n",
    "            z_i,z_j,c_i,c_j=model(x_i,x_j)\n",
    "            loss_instance=instance_loss(z_i,z_j)\n",
    "            loss_cluster=cluster_loss(c_i,c_j)\n",
    "            loss = loss_instance + loss_cluster\n",
    "            print(f'------ loss:\\n{loss}')\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "        if step % 1000 == 0:\n",
    "                print(f\"Step [{step}/{len(scDataLoader)}]\\t loss_instance: {loss_instance.item()}\\t loss_cluster: {loss_cluster.item()}\")\n",
    "            \n",
    "    return loss_epoch/iter_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "scGenDataLoader = DataLoader(scDataset, shuffle=False, batch_size=1024,drop_last=True)\n",
    "\n",
    "for features, labels in scDataLoader:\n",
    "    print(len(features[-1]))\n",
    "    print(len(features))\n",
    "    print(len(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import  save_model\n",
    "loss_device=torch.device('cuda')\n",
    "instance_loss=contrastive_loss.InstanceLoss(batch_size=args.batch_size,temperature=0.5,device=loss_device)\n",
    "cluster_loss=contrastive_loss.ClusterLoss(class_num=class_num,temperature=0.5,device=loss_device)\n",
    "accs=[]\n",
    "losses=[]\n",
    "\n",
    "memory_bank=MemoryBank(batch_size=args.batch_size,full_data=x_ndarray,topK=10)\n",
    "memory_bank.updateBank(x_ndarray)\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    loss_epoch = train(instance_loss,cluster_loss,memory_bank=memory_bank)\n",
    "    losses.append(loss_epoch/len(scDataLoader))\n",
    "    if epoch % 10 == 0:\n",
    "        save_model(args, model, optimizer, epoch)\n",
    "    print(f\"\\nEpoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(scDataLoader)} \\n\")\n",
    "    \n",
    "    if epoch %20 ==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (x, y) in enumerate(scGenDataLoader):\n",
    "                # print(type(x))\n",
    "                x=x.to('cuda')\n",
    "                embedding=model.forward_embedding(x).cpu().detach().numpy()                 \n",
    "                embeddings=np.row_stack((embeddings,embedding))\n",
    "                memory_bank.updateBank(embeddings)\n",
    "    # nmi, ari, f, acc = test()\n",
    "    # accs.append(acc)\n",
    "    # print('Test NMI = {:.4f} ARI = {:.4f} F = {:.4f} ACC = {:.4f}'.format(nmi, ari, f, acc))\n",
    "    # print('========'*8+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c12a03cda4ff7f748cca20ded5f2f563553d26b87c906453c28c9baf654f05d7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
